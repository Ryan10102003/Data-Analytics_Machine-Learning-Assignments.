{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Questions and Answers\n",
        "\n",
        "1. What is Simple Linear Regression?\n",
        " - Simple Linear Regression is a method to model the relationship between a dependent variable and one independent variable using a straight line. It predicts the dependent variable based on the linear equation: `y = mx + c`.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        " - The key assumptions are: linear relationship, independence of errors, homoscedasticity (equal variance of errors), and normally distributed errors.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        " - The coefficient **m** represents the slope of the line, indicating the change in **Y** for a one-unit change in **X**.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        " - The intercept **c** represents the value of **Y** when **X** is 0.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        " - The slope **m** is calculated using:\n",
        "`m = Σ((x - x̄)(y - ȳ)) / Σ((x - x̄)²)`\n",
        "It measures how much **Y** changes per unit of **X**.\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        " - The least squares method minimizes the sum of squared differences between actual and predicted values to find the best-fitting line.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        " - R² shows the proportion of variance in the dependent variable explained by the independent variable; values closer to 1 indicate a better fit.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        " - Multiple Linear Regression models the relationship between one dependent variable and two or more independent variables using the equation: `y = b0 + b1x1 + b2x2 + ... + bnxn`.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        " - Simple Linear Regression uses one independent variable, while Multiple Linear Regression uses two or more.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        " - Key assumptions are linearity, independence, homoscedasticity, no multicollinearity, and normally distributed errors.\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        " - Heteroscedasticity occurs when error variances are unequal, leading to inefficient estimates and unreliable hypothesis tests in regression.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        " - You can improve it by removing correlated variables, combining variables, or using techniques like Principal Component Analysis (PCA) or Ridge Regression.\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        " - Common techniques include one-hot encoding, label encoding, and creating dummy variables.\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        " - Interaction terms capture how the effect of one independent variable on the dependent variable changes with another variable.\n",
        "\n",
        "15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        " - In Simple Linear Regression, the intercept is **Y** when **X** is zero; in Multiple Linear Regression, it’s **Y** when all independent variables are zero.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        " - The slope shows how much the dependent variable changes per unit increase in the independent variable, directly affecting prediction values.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        " - The intercept gives the baseline value of the dependent variable when all predictors are zero, helping to anchor the regression line.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        " - R² doesn’t indicate if the model is appropriate, can increase with more variables regardless of relevance, and ignores prediction errors.\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        " - A large standard error means the coefficient estimate is less precise and may not be statistically significant.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        " - Heteroscedasticity appears as a funnel-shaped pattern in residual plots; addressing it is important to ensure valid standard errors and reliable inference.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        " - It means adding variables improves R² but they may not truly explain the variation, so adjusted R² penalizes for irrelevant predictors.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        " - Scaling helps standardize variables, improving model convergence and making coefficients comparable.\n",
        "\n",
        "23. What is polynomial regression?\n",
        " - Polynomial regression models the relationship between variables using a polynomial equation, allowing for curved fits beyond a straight line.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        " - Polynomial regression fits a curved line by including powers of the independent variable, while linear regression fits a straight line.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        " - Polynomial regression is used when the relationship between variables is nonlinear and can't be captured by a straight line.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        " - The general equation is:\n",
        "`y = b0 + b1x + b2x² + b3x³ + ... + bnx^n`.\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        " - Yes, by including polynomial terms of multiple variables, e.g., `y = b0 + b1x1 + b2x1² + b3x2 + b4x2²`.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        " - It can overfit easily, is sensitive to outliers, and becomes complex with high-degree polynomials.\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        " - Use cross-validation, adjusted R², and Root Mean Squared Error (RMSE) to evaluate and select the best polynomial degree.\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        " - Visualization helps understand the curve fit, detect overfitting, and assess how well the model captures the data pattern.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yQJMv7krCoHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import numpy as np # Import numpy to create sample data\n",
        "\n",
        "# Create sample data (replace with your actual data loading/generation)\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1) # Sample feature data\n",
        "y = np.array([2, 4, 5, 4, 5]) # Sample target data\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "print(\"Model training complete.\")\n",
        "print(\"Predicted values:\", y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lry5FRb_GRo7",
        "outputId": "d6b3b1e8-e8cc-4eb3-b10f-9de10607a72b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Object `Python` not found.\n",
            "Model training complete.\n",
            "Predicted values: [2.22857143 3.68571429 4.57142857 4.88571429 4.62857143]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END OF ASSIGNMENT QUESTIONS"
      ],
      "metadata": {
        "id": "EOuyGz8QGplK"
      }
    }
  ]
}